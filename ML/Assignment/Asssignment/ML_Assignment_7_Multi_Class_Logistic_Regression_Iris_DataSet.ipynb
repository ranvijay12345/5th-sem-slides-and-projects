{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Assignment - 7 Multi-Class Logistic Regression Iris DataSet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zID8FIjj6xfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f0ab8f-4abb-49df-a102-215a338d79d2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "warnings.filterwarnings('default')\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "data = load_wine()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiPGjimKilPz"
      },
      "source": [
        "y = np.array(data['target'])\n",
        "my_data = np.array(data['data'])\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(my_data)\n",
        "my_data = scaler.transform(my_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJhNNnf8U4OA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882dc6c8-a23e-4f9b-bc31-d59bf223cf5e"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "def fold(features,y_actual):\n",
        "  kf = KFold(n_splits=5,random_state=1000, shuffle=True)\n",
        "  kf.get_n_splits(features)\n",
        "  print(kf)\n",
        "  all_x_train = []\n",
        "  all_x_test = []\n",
        "  all_y_train = []\n",
        "  all_y_test = []\n",
        "  for train_index, test_index in kf.split(features):\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train, y_test = y_actual[train_index], y_actual[test_index]\n",
        "    all_x_train.append(X_train)\n",
        "    all_x_test.append(X_test)\n",
        "    all_y_train.append(y_train)\n",
        "    all_y_test.append(y_test)\n",
        "  all_x_train, all_x_test, all_y_train, all_y_test  = np.array(all_x_train), np.array(all_x_test), np.array(all_y_train), np.array(all_y_test)\n",
        "  return all_x_train, all_x_test, all_y_train, all_y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "0XgKSYtlVyLd",
        "outputId": "7773397f-0702-4ab6-948e-c0a83d588247"
      },
      "source": [
        "all_x_train, all_x_test, all_y_train, all_y_test = fold(features,y_actual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-15b8fa71c7bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_x_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_actual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs_3LpSaqk7k"
      },
      "source": [
        "class Logistic_Regression:\n",
        "    def __init__(self, lr=0.01, roh = 0.000001, num_iter=100000, fit_intercept=True, same_weight = False):\n",
        "        self.lr = lr                        # Learning Rate\n",
        "        self.num_iter = num_iter            # Maximum Iteration\n",
        "        self.roh = roh                      # Roh\n",
        "        self.fit_intercept = fit_intercept  # adding column withh all 1 entities or not\n",
        "        self.same_weight = same_weight\n",
        "    def __add_intercept(self, x):\n",
        "        intercept = np.ones((x.shape[0], 1))\n",
        "        return np.concatenate((intercept, x), axis=1)\n",
        "    def __sigmoid(self, z):                 # Sigmoid function to bound the hypothesis\n",
        "        return 1/(1 + np.exp(-z))\n",
        "    def __loss(self, h, y):                 # log-loss function\n",
        "        return (-y*np.log(h) - (1-y)*np.log(1-h)).mean()\n",
        "    def __mse(self, h, y):                  # mean square error\n",
        "        return (((h-y)**2).mean())/2\n",
        "    def predict_prob(self, x):\n",
        "        if self.fit_intercept:\n",
        "            x = self.__add_intercept(x)\n",
        "        return self.__sigmoid(np.dot(x, self.theta))\n",
        "    def predict(self, x):\n",
        "        return self.predict_prob(x).round()\n",
        "    def fit(self, x, y):\n",
        "        weight = np.zeros(x.shape[1]+1)\n",
        "        current_loss = 1                    # i.e. we have the 100% error at the intialization\n",
        "        if self.fit_intercept:              # Add the first column with all 1 entities\n",
        "            x = self.__add_intercept(x)\n",
        "        if (self.same_weight == True):      # if we want same weight intialization\n",
        "          random.seed(10)\n",
        "        for i in range(0, x.shape[1]):\n",
        "            weight[i] = random.uniform(-0.3, 0.3)\n",
        "        self.theta = weight                 # weights initialization (must be random)\n",
        "        for i in range(self.num_iter):      # convergence if we reached the max epoch and \n",
        "            z = np.dot(x, self.theta)\n",
        "            h = self.__sigmoid(z)           # hypothesis\n",
        "            gradient = np.dot(x.T, (h - y)) / y.size # new weights\n",
        "            self.theta -= self.lr * gradient\n",
        "            z = np.dot(x, self.theta)\n",
        "            h = self.__sigmoid(z)\n",
        "            loss = self.__loss(h, y)        # log loss error\n",
        "            mse_error = self.__mse(h, y)    # mse error\n",
        "            print(f'Epoch: {i} ------> ' + f'Log-Loss: {loss} ' + f' ------> Mse: {mse_error} \\t ')\n",
        "            if(abs(current_loss - loss) <= self.roh):\n",
        "              print(f\"Converged through roh criteria: epoch = {i}\")\n",
        "              break # converged\n",
        "            current_loss = loss # save the previous error to calculate the diff in current error and previous error\n",
        "            if(i + 1 == self.num_iter):\n",
        "              print(\"Converged through maximum epoch no criteria...\") # converged\n",
        "        return loss, mse_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyxO_W7PjmdW"
      },
      "source": [
        "# Calculate the mean for each fold\n",
        "print(\"Mean Accuracy: \",accuracy.mean()*100)\n",
        "print(\"Mean precission: \", precision.mean())\n",
        "print(\"Mean recall: \",recall.mean())\n",
        "print(\"Mean f1 score: \",f1.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA1o66YO-sf9"
      },
      "source": [
        "print(\"Weight of the Logistic Regression: (each row represents the class hypothesis..) \\n\" + str(all_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "immD8vKZfW6V"
      },
      "source": [
        "# Do the splitting on the first fold\n",
        "# Train test validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "# train(70%), validation (10%), and test(20%)\n",
        "X_train, X_validation, y_train, y_test = train_test_split(all_x_train[0], all_y_train[0] , test_size=0.1, random_state=random.randint(30,100))\n",
        "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.14, random_state=random.randint(30,100))\n",
        "print(\"Size of the train dataset:\" + str(X_train.shape))\n",
        "print(\"Size of the validation dataset:\" + str(X_validation.shape))\n",
        "print(\"Size of the test dataset:\" + str(X_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwqTcJ9l-sqy"
      },
      "source": [
        "# Hyperparameter tuning on the validation set is required to be done.\n",
        "# here we are choosing the best hyperparameters based on the accuracy value\n",
        "def get_best_hyperparameter(X_train, y_train, X_validation, y_validation, alpha, roh, num_iter):\n",
        "  valid_hypothesis = []\n",
        "  validation_accuracy = []\n",
        "  x = add_intercept(X_validation)\n",
        "  for i in range(0, len(alpha)):\n",
        "    all_hypothesis_valid = []\n",
        "    final_hypothesis = []\n",
        "    all_weights, final_hypothesis = logistic_regression(overfit_x, overfit_y, lr=alpha[i], roh = roh[i], num_iter=num_iter[i], same_weight = False)\n",
        "    accuracy  = (final_hypothesis == y_train).mean()*100\n",
        "    print(\"Training Accuracy: \" + str(accuracy))\n",
        "    # Now do the same weight on the validation set\n",
        "    max = 0\n",
        "    h_index = 0\n",
        "    for j in range(0, all_weights.shape[0]):\n",
        "      all_hypothesis_valid.append(sigmoid(np.dot(x, all_weights[j])))\n",
        "    all_hypothesis_valid = np.array(all_hypothesis_valid)\n",
        "    print(all_hypothesis_valid)\n",
        "    my_list = []\n",
        "    for j in range(0, all_hypothesis_valid.shape[1]): # fix the coulmn\n",
        "      max = 0\n",
        "      h_index = 0\n",
        "      for i in range(0, all_hypothesis_valid.shape[0]): # fix the row i means ith hypothesis/ ith class\n",
        "        temp = all_hypothesis_valid[i][j]\n",
        "        if (temp>max):\n",
        "          max = temp\n",
        "          h_index = i\n",
        "      # find in which hypothesis has the highest parbability value\n",
        "      my_list.append(h_index)\n",
        "    final_valid_hypothesis = np.array(my_list)\n",
        "    print(final_valid_hypothesis)\n",
        "    print(y_validation)\n",
        "    # calculate the accuracy in the validation set\n",
        "    validation_accuracy.append((final_valid_hypothesis == y_validation).mean()*100)\n",
        "  validation_accuracy = np.array(validation_accuracy)\n",
        "  max_index = np.argmax(validation_accuracy)\n",
        "  print(\"Best hyperparameter value is alpha = \" + str(alpha[max_index]) + \", roh = \" + str(roh[max_index]) + \" and num_iter = \" + str(num_iter[max_index]))\n",
        "  return validation_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oRCSjKmf8K8"
      },
      "source": [
        "alpha = [0.01, 0.0001, 0.1, 0.2]\n",
        "roh =   [0.0001, 0.0000001, 0.000000001, 0.0000000000001]\n",
        "num_iter = [10, 20, 30, 40]\n",
        "validation_accuracy = get_best_hyperparameter(X_train, y_train, X_validation, y_validation,alpha, roh, num_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjKDKX5bBpIk"
      },
      "source": [
        "def plotting(x, y_1, y_2, label_1, label_2, t):\n",
        "      plt.plot(x, y_1, label = label_1)\n",
        "      plt.plot(x, y_2, label = label_2)\n",
        "      plt.title(t)\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG5rrL7CzKLp"
      },
      "source": [
        "# Overfitting Detectcion\n",
        "class Overfitting_Detection:\n",
        "    def __init__(self, lr=0.01, roh = 0.000001, num_iter=100000, fit_intercept=True, same_weight = False):\n",
        "        self.lr = lr                        # Learning Rate\n",
        "        self.num_iter = num_iter            # Maximum Iteration\n",
        "        self.roh = roh                      # Roh\n",
        "        self.fit_intercept = fit_intercept  # adding column withh all 1 entities or not\n",
        "        self.same_weight = same_weight\n",
        "    def __add_intercept(self, x):\n",
        "        intercept = np.ones((x.shape[0], 1))\n",
        "        return np.concatenate((intercept, x), axis=1)\n",
        "    def __sigmoid(self, z):                 # Sigmoid function to bound the hypothesis\n",
        "        return 1/(1 + np.exp(-z))\n",
        "    def __loss(self, h, y):                 # log-loss function\n",
        "        return (-y*np.log(h) - (1-y)*np.log(1-h)).mean()\n",
        "    def __mse(self, h, y):                  # mean square error\n",
        "        return (((h-y)**2).mean())/2\n",
        "    def predict_prob(self, x):\n",
        "        if self.fit_intercept:\n",
        "            x = self.__add_intercept(x)\n",
        "        return self.__sigmoid(np.dot(x, self.theta))\n",
        "    def predict(self, x):\n",
        "        return self.predict_prob(x).round()\n",
        "    def check_overfitting(self, x, y, x_valid, y_valid):\n",
        "        #------------------------- list intilazation ---------------------\n",
        "        validation_accuracy = []\n",
        "        train_accuracy = []\n",
        "        validation_logloss = []\n",
        "        train_logloss = []\n",
        "        validation_mse = []\n",
        "        train_mse = []\n",
        "        plot_x = []\n",
        "        #-----------------------------------------------------------------\n",
        "        weight = np.zeros(x.shape[1]+1)\n",
        "        max_epoch = 1\n",
        "        current_loss = 1                    # i.e. we have the 100% error at the intialization\n",
        "        if self.fit_intercept:              # Add the first column with all 1 entities\n",
        "            x = self.__add_intercept(x)\n",
        "            x_valid = self.__add_intercept(x_valid)\n",
        "        if (self.same_weight == True):      # if we want same weight intialization\n",
        "          random.seed(10)\n",
        "        for i in range(0, x.shape[1]):\n",
        "            weight[i] = random.uniform(-0.3, 0.3)\n",
        "        self.theta = weight                 # weights initialization (must be random)\n",
        "        for i in range(self.num_iter):      # convergence if we reached the max epoch\n",
        "            # ------------      Train     ------------------- \n",
        "            z = np.dot(x, self.theta)\n",
        "            h = self.__sigmoid(z)           # hypothesis\n",
        "            gradient = np.dot(x.T, (h - y)) / y.size # new weights\n",
        "            self.theta -= self.lr * gradient\n",
        "            z = np.dot(x, self.theta)\n",
        "            h = self.__sigmoid(z)\n",
        "            loss = self.__loss(h, y)        # log loss error\n",
        "            mse_error = self.__mse(h, y)    # mse error\n",
        "            print(f'Epoch: {i} ------> ' + f'Log-Loss: {loss} ' + f' ------> Mse: {mse_error} \\t ')\n",
        "            if(abs(current_loss - loss) <= self.roh):\n",
        "              print(f\"Converged through roh criteria: epoch = {i}\")\n",
        "              break # converged\n",
        "            current_loss = loss # save the previous error to calculate the diff in current error and previous error\n",
        "            if(i + 1 == self.num_iter):\n",
        "              print(\"Converged through maximum epoch no criteria...\") # converged\n",
        "            # ----------------      Validation    ------------------\n",
        "            z_valid = np.dot(x_valid, self.theta)\n",
        "            h_valid = self.__sigmoid(z_valid)\n",
        "            loss_valid = self.__loss(h_valid, y_valid)        # log loss error\n",
        "            mse_error_valid = self.__mse(h_valid, y_valid)    # mse error\n",
        "            #-------------------------------------------------------------------------------\n",
        "            #-----------------------      Overfitting Detection     ------------------------\n",
        "            train_mse.append(mse_error)     # Training mse\n",
        "            validation_mse.append(mse_error_valid)  # Validation mse\n",
        "            train_logloss.append(loss)      # Training logloss\n",
        "            validation_logloss.append(loss_valid)   # Validation logloss\n",
        "            max_epoch = i\n",
        "        # plot those values\n",
        "        for i in range(0, max_epoch+1):\n",
        "          plot_x.append(i+1)\n",
        "        plotting(plot_x, validation_mse, train_mse, 'Validation MSE', 'Train MSE', 'Validation and train MSE vs Epoch')\n",
        "        plotting(plot_x, validation_logloss, train_logloss, 'Validation Log-Loss', 'Train Log-Loss', 'Validation and train Log-Loss vs Epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcYemEd7sBbN"
      },
      "source": [
        "model = Overfitting_Detection(lr=0.01, roh = 0.0001, num_iter=100, same_weight = False)\n",
        "%time model.check_overfitting(X_train, y_train, X_validation, y_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE2_WzkqclQD"
      },
      "source": [
        "# Report the classification accuracy (on test set and train set) by varying the percentage of training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLmkUpM7cl7-"
      },
      "source": [
        "# vary the test and train set\n",
        "accuracy  = []\n",
        "for i in range(1, 10):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(features,y_actual , test_size=0.1*i, random_state=random.randint(30,100))\n",
        "  model = Logistic_Regression(lr = 0.01, roh = 0.0001, num_iter = 100, same_weight = False)\n",
        "  model.fit(X_train, y_train)\n",
        "  test_prediction = model.predict(X_test)\n",
        "  accuracy.append((test_prediction == y_test).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBn-NSXkE60K"
      },
      "source": [
        "for i in range(1, 10):\n",
        "  print(f'Train:{10*i}% ' + f'Test:{100-10*i}% '+ \"Test Accuracy: \" + str(accuracy[i-1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}